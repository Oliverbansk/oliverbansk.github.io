---
---

@string{aps = {American Physical Society,}}


@inproceedings{holisticrobotpose,
    author={Ban, Shikun and Fan, Juling and Ma, Xiaoxuan and Zhu, Wentao and Qiao, Yu and Wang, Yizhou},
    title={Real-time Holistic Robot Pose Estimation with Unknown States},
    abstract={Estimating robot pose from RGB images is a crucial problem in computer vision and robotics. While previous methods have achieved promising performance, most of them presume full knowledge of robot internal states, e.g. ground-truth robot joint angles. However, this assumption is not always valid in practical situations. In real-world applications such as multi-robot collaboration or human-robot interaction, the robot joint states might not be shared or could be unreliable. On the other hand, existing approaches that estimate robot pose without joint state priors suffer from heavy computation burdens and thus cannot support real-time applications. This work introduces an efficient framework for real-time robot pose estimation from RGB images without requiring known robot states. Our method estimates camera-to-robot rotation, robot state parameters, keypoint locations, and root depth, employing a neural network module for each task to facilitate learning and sim-to-real transfer. Notably, it achieves inference in a single feed-forward pass without iterative optimization. Our approach offers a 12-time speed increase with state-of-the-art accuracy, enabling real-time holistic robot pose estimation for the first time. },
    website={https://oliverbansk.github.io/Holistic-Robot-Pose/},
    arxiv={2402.05655},
    video={https://www.youtube.com/watch?v=9NsLJvp1IPE},
    code={https://github.com/Oliverbansk/Holistic-Robot-Pose-Estimation},
    pdf={holisticrobotpose.pdf},
    booktitle = {European Conference on Computer Vision (ECCV)},
    year = {2024},
    preview = {holisticrobotpose.gif},
    selected={true}
}

@inproceedings{humanawarerobot,
    author={Qin, Jason and Ban, Shikun and Zhu, Wentao and Wang, Yizhou and Dimitris, Samaras},
    title={Learning Human-aware Robot Policies for Adaptive Assistance},
    abstract={Developing robots that can assist humans efficiently, safely, and adaptively is crucial for real-world applications such as healthcare. While previous work often assumes a centralized system for co-optimizing human-robot interactions, we argue that real-world scenarios are much more complicated, as humans have individual preferences regarding how tasks are performed. Robots typically lack direct access to these implicit preferences. However, to provide effective assistance, robots must still be able to recognize and adapt to the individual needs and preferences of different users. To address these challenges, we propose a novel framework in which robots infer human intentions and reason about human utilities through interaction. Our approach features two critical modules: the anticipation module is a motion predictor that captures the spatial-temporal relationship between the robot agent and user agent, which contributes to predicting human behavior; the utility module infers the underlying human utility functions through progressive task demonstration sampling. Extensive experiments across various robot types and assistive tasks demonstrate that the proposed framework not only enhances task success and efficiency but also significantly improves user satisfaction, paving the way for more personalized and adaptive assistive robotic systems.},
    website={https://asonin.github.io/Human-Aware-Assistance/},
    arxiv={2412.11913},
    video={https://youtu.be/yNTk-Wt6feA},
    code={https://github.com/Asonin/Human-Aware-Assistance-Codespace},
    pdf={humanawarerobot.pdf},
    booktitle = {arXiv preprint arXiv:2412.11913},
    year = {2024},
    preview = {humanawarerobot_v1.gif},
    selected={true}
}
